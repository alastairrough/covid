{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid: Using scrapy regex xpath\n",
    "\n",
    "## References\n",
    "1. https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.\n",
    "1. https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "### Method/Attribute\n",
    "\n",
    "```match()```\n",
    "Determine if the RE matches at the beginning of the string.\n",
    "\n",
    "```search()```\n",
    "Scan through a string, looking for any location where this RE matches.\n",
    "\n",
    "```findall()```\n",
    "Find all substrings where the RE matches, and returns them as a list.\n",
    "\n",
    "```finditer()```\n",
    "Find all substrings where the RE matches, and returns them as an iterator.\n",
    "\n",
    "and more e.g. sub\n",
    "\n",
    "```\n",
    "prog = re.compile(pattern)\n",
    "result = prog.match(string)\n",
    "```\n",
    "is equivalent to\n",
    "\n",
    "```\n",
    "result = re.match(pattern, string)\n",
    "```\n",
    "but using re.compile() and saving the resulting regular expression object for reuse is more efficient when the expression will be used several times in a single program.\n",
    "\n",
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s1 = '<p class=\"\">12:05 PM<br>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1\n",
    "type(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern doesnt match with start of string, so ```match``` method returns empty \n",
    "p.match(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.search(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.findall(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pattern match with start of string, so ```search``` method returns Match object with all matching empty \n",
    "# using a pattern that matches\n",
    "m1 = re.match('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','12:05 PM' )\n",
    "print(m1)\n",
    "print(m1.span())\n",
    "print(m1.group())\n",
    "print(m1.string)\n",
    "print(type(m1))\n",
    "print(isinstance(m1,re.Match))\n",
    "for i in range(5):\n",
    "    print(i,m1.group(i))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pattern match with start of string, so ```search``` method returns Match object with all matching empty \n",
    "m2 = re.search('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' )\n",
    "print(m2)\n",
    "print(m2.span())\n",
    "print(m2.string)\n",
    "print(m2.group())\n",
    "print(type(m2))\n",
    "print(isinstance(m2,re.Match))\n",
    "for i in range(5):\n",
    "    print(i,m2.group(i))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pattern match with start of string, so ```findall``` method returns Match object with all matching empty \n",
    "m3 = re.findall('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' )\n",
    "print(m3)\n",
    "# print(m3.span())    # 'list' object has no attribute 'span'\n",
    "# print(m3.string)    # 'list' object has no attribute 'string'\n",
    "# print(m3.match())     #'list' object has no attribute 'match'\n",
    "print(type(m3))\n",
    "print(isinstance(m3,list))\n",
    "print(i,m3[0][0])\n",
    "\n",
    "for i in range(4):\n",
    "    print(i,m3[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','12:05 PM' ).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','12:05 PM' ).group() # or group(0), group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' ).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' ).group()  # or group(0), group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiled pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.match(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.search(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.findall(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' ).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))','<p class=\"\">12:05 PM<br>' ).group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xpath  \n",
    "The schema of the web pages for public and schools exposures differ in HTML structure, and neither is internally consistent. So the xpath selectors differ.\n",
    "\n",
    "### Running scrapy in notebook\n",
    "Under the Files tab open a new terminal: New > Terminal\n",
    "Then simply run you spider: scrapy crawl [options] <spider>\n",
    "\n",
    "2. Create a new notebook and use CrawlerProcess or CrawlerRunner classes to run in a cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "# scrape public exposures\n",
    "!scrapy crawl covid01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape school exposures\n",
    "!scrapy crawl covid02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School exposures output to JSON\n",
    "This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "# restart kernel\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "import scrapy\n",
    "# except:\n",
    "#     !pip install scrapy\n",
    "#     import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('schoolsresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from covid03_spider.py\n",
    "parses local copy of VCH convid 19 exposures\n",
    "school exposures\n",
    "outputs to json file\n",
    "\"\"\"\n",
    "import scrapy\n",
    "import logging\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "    \n",
    "# class QuotesSpider(scrapy.Spider):\n",
    "class covid_schools(scrapy.Spider):\n",
    "    name = \"covid03\"\n",
    "    start_urls = [\n",
    "        'http://localhost/schools_exposures.html',\n",
    "#         'http://www.vch.ca/covid-19/school-exposures',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        # Used for pipeline 1\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
    "        'FEED_FORMAT': 'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'schoolsresult.json'                        # Used for pipeline 2\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        use xpath response\n",
    "        \"\"\"\n",
    "        # scrapy regex outputs all match groups as strings\n",
    "        for myMatch in response.xpath('//*[@id=\"809\"]/div/div//span/text()').getall():\n",
    "        # for myMatch in response.css('div.table-responsive > table > tbody > tr > td:nth-child(1) > p::text').re(r'(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))'):\n",
    "                print(myMatch)\n",
    "                ######\n",
    "                yield {\n",
    "                     'school': myMatch,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# process.crawl(QuotesSpider)\n",
    "process.crawl(covid_schools)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll *.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll schoolsresult*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 schoolsresult.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 schoolsresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more schoolsresult.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail -n 2 schoolsresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more schoolsresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public exposures output to JSON\n",
    "This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "# restart kernel\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "import scrapy\n",
    "# except:\n",
    "#     !pip install scrapy\n",
    "#     import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('publicresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom covid04_spider.py\\nparses local copy of VCH convid 19 exposures\\nschool exposures\\noutputs to json file\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from covid04_spider.py\n",
    "parses local copy of VCH convid 19 exposures\n",
    "school exposures\n",
    "outputs to json file\n",
    "\"\"\"\n",
    "import scrapy\n",
    "import logging\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "    \n",
    "# class QuotesSpider(scrapy.Spider):\n",
    "class covid_public(scrapy.Spider):\n",
    "    name = \"covid04\"\n",
    "    start_urls = [\n",
    "            'http://localhost/public_exposures.html',\n",
    "#             'http://www.vch.ca/covid-19/public-exposures',\n",
    "     ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        # Used for pipeline 1\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
    "        'FEED_FORMAT': 'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'publicresult.json'                        # Used for pipeline 2\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        use xpath response\n",
    "        \"\"\"\n",
    "        # scrapy regex outputs all match groups as strings\n",
    "        for myMatch in response.xpath('//*[@id=\"9184\"]/div').re(r'<span style=\"font-size:14px;\">(.*?)<\\/span>'):\n",
    "        # for myMatch in response.xpath('//*[@id=\"809\"]/div/div//span/text()').getall():\n",
    "        # for myMatch in response.css('div.table-responsive > table > tbody > tr > td:nth-child(1) > p::text').re(r'(([01]?[0-9]):([0-5][0-9]) ([AaPp][Mm]))'):\n",
    "            myMatch1 = remove_html_tags(myMatch)\n",
    "            print(myMatch1)\n",
    "            ######\n",
    "            yield {\n",
    "                 'public': myMatch1,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-02 21:35:41 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-10-02 21:35:41 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.8 (default, Aug 17 2020, 15:18:11) - [GCC 5.4.0 20160609], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-4.15.0-118-generic-x86_64-with-debian-stretch-sid\n",
      "2020-10-02 21:35:41 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-10-02 21:35:41 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-10-02 21:35:41 [py.warnings] WARNING: /home/alastair/.local/lib/python3.7/site-packages/scrapy/extensions/feedexport.py:239: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7f9d31889a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Park Drive\n",
      "Address: 1815 Commercial Drive, Vancouver, BC\n",
      "Potential exposure date(s): \n",
      "September 26\n",
      "Potential exposure time: Between 6:00 p.m. and 10:00 p.m.\n",
      "\n",
      "Abruzzo Cappuccino Bar\n",
      "Address: 1321 Commercial Drive, Vancouver, BC\n",
      "Potential exposure date(s): \n",
      "September 23 to 26\n",
      "Potential exposure time: Between 1:00 p.m. and 3:00 p.m.\n",
      "\n",
      "Wreck Beach\n",
      "Address: Southwest Marine Drive, Vancouver, BC\n",
      "Potential exposure date(s): \n",
      "September 7\n",
      "Potential exposure time: Between 1:00 p.m. and 8:30 p.m.\n",
      "\n",
      "\n",
      "The King’s Head Public House\n",
      "Address: 1618 Yew Street, Vancouver, BC\n",
      "Potential exposure date(s): \n",
      "September 4 to September 7\n",
      "Potential exposure \n",
      "\n",
      "\n",
      "Athens Cultural Club\n",
      "Address: 114 West Broadway, Vancouver, BC\n",
      "Potential exposure date(s): \n",
      "August 26 to September 8\n",
      "Potential exposure \n",
      "\n",
      "\n",
      "The West Pub\n",
      "Address: 488 Carrall Street, Vancouver\n",
      "Potential exposure date(s): \n",
      "August 20 to September 8\n",
      "Potential exposure \n",
      "\n",
      "\n",
      "Flying Beaver Bar and Grill\n",
      "Address: 4760 Inglis Drive, Richmond\n",
      "Potential exposure date(s): \n",
      "August 28 to September 3\n",
      "Potential exposure \n",
      "‎\n",
      "*Locations will be removed from the list one month after the last exposure date, and then archived.\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# process.crawl(QuotesSpider)\n",
    "process.crawl(covid_public)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 alastair 1900 Oct  2 21:35 publicresult.jl\r\n",
      "-rw-rw-r-- 1 alastair 2581 Oct  2 21:34 schoolsresult.jl\r\n"
     ]
    }
   ],
   "source": [
    "ll *.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 alastair 1949 Oct  2 21:35 publicresult.json\r\n",
      "-rw-rw-r-- 1 alastair 2632 Oct  2 21:34 schoolsresult.json\r\n"
     ]
    }
   ],
   "source": [
    "ll *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 alastair 1900 Oct  2 21:35 publicresult.jl\r\n",
      "-rw-rw-r-- 1 alastair 1949 Oct  2 21:35 publicresult.json\r\n"
     ]
    }
   ],
   "source": [
    "ll publicresult*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"public\": \"\\u200e\"}\r\n",
      "{\"public\": \"*Locations will be removed from the list one month after the last exposure date, and then archived.\"}\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 publicresult.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"public\": \"*Locations will be removed from the list one month after the last exposure date, and then archived.\"}\r\n",
      "]"
     ]
    }
   ],
   "source": [
    "!tail -n 2 publicresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"public\": \"Park Drive\"}\r\n",
      "{\"public\": \"Address: 1815\\u00a0Commercial Drive, Vancouver, BC\"}\r\n",
      "{\"public\": \"Potential exposure date(s):\\u00a0\"}\r\n",
      "{\"public\": \"September 26\"}\r\n",
      "{\"public\": \"Potential exposure time: Between 6:00 p.m. and 10:00 p.m.\"}\r\n",
      "{\"public\": \"\"}\r\n",
      "{\"public\": \"Abruzzo Cappuccino Bar\"}\r\n",
      "{\"public\": \"Address: 1321 Commercial Drive, Vancouver, BC\"}\r\n",
      "{\"public\": \"Potential exposure date(s):\\u00a0\"}\r\n",
      "{\"public\": \"September 23 to 26\"}\r\n",
      "{\"public\": \"Potential exposure time: Between 1:00 p.m. and 3:00 p.m.\"}\r\n",
      "{\"public\": \"\"}\r\n",
      "{\"public\": \"Wreck Beach\"}\r\n",
      "{\"public\": \"Address: Southwest Marine Drive, Vancouver, BC\"}\r\n",
      "{\"public\": \"Potential exposure date(s):\\u00a0\"}\r\n",
      "{\"public\": \"September 7\"}\r\n",
      "{\"public\": \"Potential exposure time: Between 1:00 p.m. and 8:30 p.m.\"}\r\n",
      "{\"public\": \"\"}\r\n",
      "{\"public\": \"\"}\r\n",
      "{\"public\": \"The King\\u2019s Head Public House\"}\r\n",
      "{\"public\": \"Address:\\u00a01618 Yew Street, Vancouver, BC\"}\r\n",
      "{\"public\": \"Potential exposure date(s):\\u00a0\"}\r\n",
      "{\"public\": \"September 4 to September 7\"}\r\n",
      "\u001b[7m--More--(51%)\u001b[m"
     ]
    }
   ],
   "source": [
    "!more publicresult.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail -n 2 publicresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!more publicresult.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
